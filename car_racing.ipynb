{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import gymnasium as gym\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import cv2\n",
    "from gymnasium.spaces import Box\n",
    "\n",
    "def preprocess(img):\n",
    "    img = img[:84, 6:90] # CarRacing-v2-specific cropping\n",
    "    img = cv2.resize(img, dsize=(84, 84)) # or you can simply use rescaling\n",
    "\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) / 255.0\n",
    "    return img\n",
    "\n",
    "class WrapperEnv(gym.Wrapper):\n",
    "    def __init__(\n",
    "        self,\n",
    "        env,\n",
    "        skip_frames=4,\n",
    "        stack_frames=4,\n",
    "        initial_no_op=50,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(WrapperEnv, self).__init__(env, **kwargs)\n",
    "        self.initial_no_op = initial_no_op\n",
    "        self.skip_frames = skip_frames\n",
    "        self.stack_frames = stack_frames\n",
    "        \n",
    "        self.observation_space = Box(\n",
    "            low=0,\n",
    "            high=255,\n",
    "            shape=(4, 84, 84),\n",
    "            dtype=np.uint8\n",
    "        )\n",
    "    def reset(self):\n",
    "        # Reset the original environment.\n",
    "        s, info = self.env.reset()\n",
    "\n",
    "        # Do nothing for the next `self.initial_no_op` steps\n",
    "        for i in range(self.initial_no_op):\n",
    "            s, r, terminated, truncated, info = self.env.step(0)\n",
    "\n",
    "        # Convert a frame to 84 X 84 gray scale one\n",
    "        s = preprocess(s)\n",
    "\n",
    "        # The initial observation is simply a copy of the frame `s`\n",
    "        self.stacked_state = np.tile(s, (self.stack_frames, 1, 1))  # [4, 84, 84]\n",
    "        return self.stacked_state, info\n",
    "\n",
    "    def step(self, action):\n",
    "        # We take an action for self.skip_frames steps\n",
    "        reward = 0\n",
    "        for _ in range(self.skip_frames):\n",
    "            s, r, terminated, truncated, info = self.env.step(action)\n",
    "            reward += r\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        # Convert a frame to 84 X 84 gray scale one\n",
    "        s = preprocess(s)\n",
    "\n",
    "        # Push the current frame `s` at the end of self.stacked_state\n",
    "        self.stacked_state = np.concatenate((self.stacked_state[1:], s[np.newaxis]), axis=0)\n",
    "\n",
    "        return self.stacked_state, reward, terminated, truncated, info\n",
    "\n",
    "        # Preprocess the observation (permute dimensions)\n",
    "        obs = self._preprocess(obs)\n",
    "        return obs\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61d94855858ea8c8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, activation=F.relu):\n",
    "        super(Actor, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(state_dim, 16, kernel_size=8, stride=4)  # [N, 4, 84, 84] -> [N, 16, 20, 20]\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=4, stride=2)  # [N, 16, 20, 20] -> [N, 32, 9, 9]\n",
    "        self.in_features = 32 * 9 * 9\n",
    "        self.fc1 = nn.Linear(self.in_features, 256)\n",
    "        self.fc2 = nn.Linear(256, action_dim)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view((-1, self.in_features))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        distribution = Categorical(logits=x)\n",
    "        return distribution\n",
    "        \n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, activation=F.relu):\n",
    "        super(Critic, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(state_dim, 16, kernel_size=8, stride=4)  # [N, 4, 84, 84] -> [N, 16, 20, 20]\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=4, stride=2)  # [N, 16, 20, 20] -> [N, 32, 9, 9]\n",
    "        self.in_features = 32 * 9 * 9\n",
    "        self.fc1 = nn.Linear(self.in_features, 256)\n",
    "        self.fc2 = nn.Linear(256, 1)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view((-1, self.in_features))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "254bb23f9cdff2a1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from ppo import PPO\n",
    "env = gym.make(\"CarRacing-v2\", continuous=False)\n",
    "env = WrapperEnv(env)\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.n\n",
    "\n",
    "actor = Actor(4, 5).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "critic = Critic(4).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "agent = PPO(env, actor, critic)\n",
    "\n",
    "n_iterations = 1000\n",
    "n_eps_per_iter = 10\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "439e1a851d95f58d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "agent.train(n_iters=1000)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be8ddbccd474c8ef"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(agent.score_history)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f34e845dfba22c7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2bfa698c1112cd17"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
